{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e266e36-a79f-4085-8b9d-aef6b243cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azureuser/Documents\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af401d11-ffe7-4d97-b787-8b5b121693d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9660fb4e-030d-4269-8289-cc8e92c5b205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.jp-OutputArea-output pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.jp-OutputArea-output pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb678df-cb03-4de8-b912-5889161cef8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a Spark Session CSV\n",
      "**Created\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "print(\"Create a Spark Session CSV\")\n",
    "spark_hadoop = SparkSession.builder.appName(\"HadoopCSVAnalysis\").getOrCreate()\n",
    "print(\"**Created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47a16cea-970a-413c-8a25-f453d36b62fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 16:59:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o82.load.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m url \u001b[38;5;241m=\u001b[39m server_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabaseName=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m database_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus_energy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m jdbcDF \u001b[38;5;241m=\u001b[39m \u001b[43mspark_sqlserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.microsoft.sqlserver.jdbc.spark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.microsoft.sqlserver.jdbc.SQLServerDriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMyP@55word\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m jdbcDF\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o82.load.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "spark_sqlserver = SparkSession.builder.appName(\"FirstRun\")\\\n",
    ".config(\"spark.jars.packages\", \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "server_name = \"jdbc:sqlserver://localhost:1433\"\n",
    "database_name = \"project_db\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "table_name = \"us_energy\"\n",
    "jdbcDF = spark_sqlserver.read\\\n",
    ".format(\"com.microsoft.sqlserver.jdbc.spark\")\\\n",
    ".option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    ".option(\"url\", url)\\\n",
    ".option(\"dbtable\", table_name)\\\n",
    ".option(\"user\", \"sa\")\\\n",
    ".option(\"password\", \"MyP@55word\").load()\n",
    "\n",
    "\n",
    "\n",
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6389df13-ad95-4200-b82f-cfdaf6ce5df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 08:23:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark_mongo = SparkSession.builder \\\n",
    "  .appName(\"MongoDB Integration\") \\\n",
    "  .config(\"spark.mongodb.input.uri\", \"mongodb+srv://jagaban:2oVlVRV3nxWDfi2L@bigdata.4z1fx3p.mongodb.net/birth_data.canadian_births\") \\\n",
    "  .config(\"spark.mongodb.output.uri\", \"mongodb+srv://jagaban:2oVlVRV3nxWDfi2L@bigdata.4z1fx3p.mongodb.net/birth_data.canadian_births\") \\\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab68179-7aed-4bcf-8f4e-a2c153bf7525",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Missing 'uri' property from options",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m ca_births \u001b[38;5;241m=\u001b[39m \u001b[43mspark_mongo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.mongodb.spark.sql.DefaultSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatabase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbirth_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcanadian_births\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Missing 'uri' property from options"
     ]
    }
   ],
   "source": [
    "ca_births = spark_mongo.read \\\n",
    "                 .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "                 .option(\"database\", \"birth_data\") \\\n",
    "                 .option(\"collection\", \"canadian_births\") \\\n",
    "                 .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d994cde6-48e0-4d7a-bb95-36212a34bcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+-----------+--------+---------------+------+-----------------+-----------+----------------+------------+---------+---------+-------------+------+---------+--------+---------+----------+--------------------+\n",
      "|CA_Births|CA_COORDINATE|  CA_Characteristics|CA_DECIMALS|CA_DGUID|CA_Day_of_birth|CA_GEO|CA_Month_of_birth|CA_REF_DATE|CA_SCALAR_FACTOR|CA_SCALAR_ID|CA_STATUS|CA_SYMBOL|CA_TERMINATED|CA_UOM|CA_UOM_ID|CA_VALUE|CA_VECTOR|      Date|                 _id|\n",
      "+---------+-------------+--------------------+-----------+--------+---------------+------+-----------------+-----------+----------------+------------+---------+---------+-------------+------+---------+--------+---------+----------+--------------------+\n",
      "|   1083.0|          NaN|Number of live bi...|        NaN|     NaN|            1.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-01|{668f3ff55385940a...|\n",
      "|   1015.0|          NaN|Number of live bi...|        NaN|     NaN|            2.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-02|{668f3ff55385940a...|\n",
      "|   1016.0|          NaN|Number of live bi...|        NaN|     NaN|            3.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-03|{668f3ff55385940a...|\n",
      "|   1051.0|          NaN|Number of live bi...|        NaN|     NaN|            4.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-04|{668f3ff55385940a...|\n",
      "|    973.0|          NaN|Number of live bi...|        NaN|     NaN|            5.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-05|{668f3ff55385940a...|\n",
      "|   1058.0|          NaN|Number of live bi...|        NaN|     NaN|            6.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-06|{668f3ff55385940a...|\n",
      "|    997.0|          NaN|Number of live bi...|        NaN|     NaN|            7.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-07|{668f3ff55385940a...|\n",
      "|   1071.0|          NaN|Number of live bi...|        NaN|     NaN|            8.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-08|{668f3ff55385940a...|\n",
      "|    989.0|          NaN|Number of live bi...|        NaN|     NaN|            9.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-09|{668f3ff55385940a...|\n",
      "|   1011.0|          NaN|Number of live bi...|        NaN|     NaN|           10.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-10|{668f3ff55385940a...|\n",
      "|   1042.0|          NaN|Number of live bi...|        NaN|     NaN|           11.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-11|{668f3ff55385940a...|\n",
      "|   1070.0|          NaN|Number of live bi...|        NaN|     NaN|           12.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-12|{668f3ff55385940a...|\n",
      "|   1045.0|          NaN|Number of live bi...|        NaN|     NaN|           13.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-13|{668f3ff55385940a...|\n",
      "|   1066.0|          NaN|Number of live bi...|        NaN|     NaN|           14.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-14|{668f3ff55385940a...|\n",
      "|   1064.0|          NaN|Number of live bi...|        NaN|     NaN|           15.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-15|{668f3ff55385940a...|\n",
      "|   1040.0|          NaN|Number of live bi...|        NaN|     NaN|           16.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-16|{668f3ff55385940a...|\n",
      "|   1069.0|          NaN|Number of live bi...|        NaN|     NaN|           17.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-17|{668f3ff55385940a...|\n",
      "|   1016.0|          NaN|Number of live bi...|        NaN|     NaN|           18.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-18|{668f3ff55385940a...|\n",
      "|   1064.0|          NaN|Number of live bi...|        NaN|     NaN|           19.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-19|{668f3ff55385940a...|\n",
      "|   1071.0|          NaN|Number of live bi...|        NaN|     NaN|           20.0|   NaN|          January|       1991|             NaN|         NaN|      NaN|      NaN|          NaN|   NaN|      NaN|     NaN|      NaN|1991-01-20|{668f3ff55385940a...|\n",
      "+---------+-------------+--------------------+-----------+--------+---------------+------+-----------------+-----------+----------------+------------+---------+---------+-------------+------+---------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#ca_births = spark_mongo.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", \"mongodb+srv://jagaban:2oVlVRV3nxWDfi2L@bigdata.4z1fx3p.mongodb.net/birth_data.canadian_births\").load()\n",
    "ca_births.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53974619-0067-4051-9497-3c0f5c6111be",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 08:49:43 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)\n",
      "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Zero blocklocations for /user/input/project/USA_Crimes_-_2001_to_Present.csv. Name node is in safe mode.\n",
      "Resources are low on NN. Please add or free up more resourcesthen turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off. NamenodeHostName:localhost\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1612)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2131)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n",
      "\tat com.sun.proxy.$Proxy34.getBlockLocations(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:333)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy35.getBlockLocations(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:900)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:889)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:878)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1046)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:340)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:336)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:353)\n",
      "\tat org.apache.hadoop.fs.FileSystem.lambda$openFileWithOptions$0(FileSystem.java:4633)\n",
      "\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n",
      "\tat org.apache.hadoop.fs.FileSystem.openFileWithOptions(FileSystem.java:4631)\n",
      "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.$anonfun$readToUnsafeMem$1(TextFileFormat.scala:119)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/07/11 08:49:43 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (myvm.internal.cloudapp.net executor driver): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Zero blocklocations for /user/input/project/USA_Crimes_-_2001_to_Present.csv. Name node is in safe mode.\n",
      "Resources are low on NN. Please add or free up more resourcesthen turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off. NamenodeHostName:localhost\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1612)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2131)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n",
      "\tat com.sun.proxy.$Proxy34.getBlockLocations(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:333)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy35.getBlockLocations(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:900)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:889)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:878)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1046)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:340)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:336)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:353)\n",
      "\tat org.apache.hadoop.fs.FileSystem.lambda$openFileWithOptions$0(FileSystem.java:4633)\n",
      "\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n",
      "\tat org.apache.hadoop.fs.FileSystem.openFileWithOptions(FileSystem.java:4631)\n",
      "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.$anonfun$readToUnsafeMem$1(TextFileFormat.scala:119)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/07/11 08:49:43 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o73.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (myvm.internal.cloudapp.net executor driver): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Zero blocklocations for /user/input/project/USA_Crimes_-_2001_to_Present.csv. Name node is in safe mode.\nResources are low on NN. Please add or free up more resourcesthen turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off. NamenodeHostName:localhost\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1612)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2131)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy34.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:333)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy35.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:900)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:889)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:878)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1046)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:340)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:336)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:353)\n\tat org.apache.hadoop.fs.FileSystem.lambda$openFileWithOptions$0(FileSystem.java:4633)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.FileSystem.openFileWithOptions(FileSystem.java:4631)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.$anonfun$readToUnsafeMem$1(TextFileFormat.scala:119)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Zero blocklocations for /user/input/project/USA_Crimes_-_2001_to_Present.csv. Name node is in safe mode.\nResources are low on NN. Please add or free up more resourcesthen turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off. NamenodeHostName:localhost\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1612)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2131)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy34.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:333)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy35.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:900)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:889)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:878)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1046)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:340)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:336)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:353)\n\tat org.apache.hadoop.fs.FileSystem.lambda$openFileWithOptions$0(FileSystem.java:4633)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.FileSystem.openFileWithOptions(FileSystem.java:4631)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.$anonfun$readToUnsafeMem$1(TextFileFormat.scala:119)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the data from Hadoop\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m USA_Crimes \u001b[38;5;241m=\u001b[39m \u001b[43mspark_hadoop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://localhost:9000/user/input/project/USA_Crimes_-_2001_to_Present.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ca_temp \u001b[38;5;241m=\u001b[39m spark_hadoop\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://localhost:9000/user/input/project/CA_temperature.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o73.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (myvm.internal.cloudapp.net executor driver): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Zero blocklocations for /user/input/project/USA_Crimes_-_2001_to_Present.csv. Name node is in safe mode.\nResources are low on NN. Please add or free up more resourcesthen turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off. NamenodeHostName:localhost\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1612)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2131)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy34.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:333)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy35.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:900)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:889)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:878)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1046)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:340)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:336)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:353)\n\tat org.apache.hadoop.fs.FileSystem.lambda$openFileWithOptions$0(FileSystem.java:4633)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.FileSystem.openFileWithOptions(FileSystem.java:4631)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.$anonfun$readToUnsafeMem$1(TextFileFormat.scala:119)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Zero blocklocations for /user/input/project/USA_Crimes_-_2001_to_Present.csv. Name node is in safe mode.\nResources are low on NN. Please add or free up more resourcesthen turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off. NamenodeHostName:localhost\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1612)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2131)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy34.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:333)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy35.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:900)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:889)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:878)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1046)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:340)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:336)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:353)\n\tat org.apache.hadoop.fs.FileSystem.lambda$openFileWithOptions$0(FileSystem.java:4633)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.FileSystem.openFileWithOptions(FileSystem.java:4631)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat.$anonfun$readToUnsafeMem$1(TextFileFormat.scala:119)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Load the data from Hadoop\n",
    "USA_Crimes = spark_hadoop.read.csv(\"hdfs://localhost:9000/user/input/project/USA_Crimes_-_2001_to_Present.csv\", header=True)\n",
    "ca_temp = spark_hadoop.read.csv(\"hdfs://localhost:9000/user/input/project/CA_temperature.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8466e9f3-35f4-4ac1-8c95-9296a484af35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+---------+----------+--------------------+\n",
      "|      ID|Case_Number|      Date|               Block|IUCR|        Primary_Type|         Description|Location_Description|Arrest|Domestic|Beat|District|Ward|Community_Area|FBI_Code|X_Coordinate|Y_Coordinate|Year|          Updated_On| Latitude| Longitude|            Location|\n",
      "+--------+-----------+----------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+---------+----------+--------------------+\n",
      "| 6995825|   HR401908|2001-01-01|050XX W BERENICE AVE|0841|               THEFT|FINANCIAL ID THEF...|           APARTMENT| False|   False|1634|    16.0|45.0|          15.0|      06|   1141979.0|   1925160.0|2001|07/04/2009 01:04:...|41.950703|-87.753494|(41.95070415, -87...|\n",
      "| 9387957|   HW531459|2001-01-01|  082XX W ADDISON ST|0840|               THEFT|FINANCIAL ID THEF...|           RESIDENCE| False|   False|1631|    16.0|36.0|          17.0|      06|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "| 9755149|   HX403546|2001-01-01|031XX W DOUGLAS BLVD|1562|         SEX OFFENSE|AGG CRIMINAL SEXU...|CHURCH/SYNAGOGUE/...| False|   False|1022|    10.0|24.0|          29.0|      17|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "| 8602342|   HV276481|2001-01-01|    085XX S GREEN ST|1754|OFFENSE INVOLVING...|AGG SEX ASSLT OF ...|           RESIDENCE| False|   False| 613|     6.0|21.0|          71.0|      02|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "|11386636|   JB357844|2001-01-01|057XX S ARTESIAN AVE|1153|  DECEPTIVE PRACTICE|FINANCIAL IDENTIT...|           RESIDENCE| False|   False| 824|     8.0|16.0|          63.0|      11|         0.0|         0.0|2001|07/21/2018 03:49:...|      0.0|       0.0|                   0|\n",
      "| 8846706|   HV519872|2001-01-01|  104XX S NORMAL AVE|1754|OFFENSE INVOLVING...|AGG SEX ASSLT OF ...|           RESIDENCE| False|    True|2233|    22.0|34.0|          49.0|      02|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "| 9748492|   HX397219|2001-01-01|031XX W DOUGLAS BLVD|1562|         SEX OFFENSE|AGG CRIMINAL SEXU...|CHURCH/SYNAGOGUE/...| False|   False|1022|    10.0|24.0|          29.0|      17|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "| 8870207|   HV543569|2001-01-01|   003XX S DAMEN AVE|0841|               THEFT|FINANCIAL ID THEF...|               OTHER| False|   False|1211|    12.0| 2.0|          28.0|      06|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "|10505529|   HZ247066|2001-01-01|    104XX S AVENUE F|1153|  DECEPTIVE PRACTICE|FINANCIAL IDENTIT...|           RESIDENCE| False|   False| 432|     4.0|10.0|          52.0|      11|         0.0|         0.0|2001|05/07/2016 03:48:...|      0.0|       0.0|                   0|\n",
      "| 8623828|   HV297549|2001-01-01|046XX S HERMITAGE...|0266| CRIM SEXUAL ASSAULT|           PREDATORY|           RESIDENCE| False|   False| 924|     9.0|20.0|          61.0|      02|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "| 9755245|   HX403558|2001-01-01|031XX W DOUGLAS BLVD|1562|         SEX OFFENSE|AGG CRIMINAL SEXU...|CHURCH/SYNAGOGUE/...| False|   False|1022|    10.0|24.0|          29.0|      17|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "| 9204813|   HW344902|2001-01-01|107XX S CHAMPLAIN...|1562|         SEX OFFENSE|AGG CRIMINAL SEXU...|           RESIDENCE| False|   False| 513|     5.0| 9.0|          50.0|      17|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "|11295508|   JB237608|2001-01-01| 068XX S ROCKWELL ST|1153|  DECEPTIVE PRACTICE|FINANCIAL IDENTIT...|           RESIDENCE| False|   False| 831|     8.0|17.0|          66.0|      11|         0.0|         0.0|2001|10/27/2018 04:00:...|      0.0|       0.0|                   0|\n",
      "| 9442043|   HW586150|2001-01-01|008XX S INDEPENDE...|0266| CRIM SEXUAL ASSAULT|           PREDATORY|           RESIDENCE| False|    True|1133|    11.0|24.0|          26.0|      02|         0.0|         0.0|2001|09/07/2021 03:41:...|      0.0|       0.0|                   0|\n",
      "| 9755238|   HX403557|2001-01-01|031XX W DOUGLAS BLVD|1562|         SEX OFFENSE|AGG CRIMINAL SEXU...|CHURCH/SYNAGOGUE/...| False|   False|1022|    10.0|24.0|          29.0|      17|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "|12413905|   JE289692|2001-01-01|   048XX S THROOP ST|1752|OFFENSE INVOLVING...|AGGRAVATED CRIMIN...|           RESIDENCE| False|    True| 933|     9.0|20.0|          61.0|      17|         0.0|         0.0|2001|07/10/2021 03:40:...|      0.0|       0.0|                   0|\n",
      "| 9652075|   HX302422|2001-01-01|   033XX W WALNUT ST|0266| CRIM SEXUAL ASSAULT|           PREDATORY|           RESIDENCE| False|    True|1123|    11.0|28.0|          27.0|      02|         0.0|         0.0|2001|09/07/2021 03:41:...|      0.0|       0.0|                   0|\n",
      "|11513580|   JB524424|2001-01-01| 030XX W WARREN BLVD|1753|OFFENSE INVOLVING...|SEXUAL ASSAULT OF...|           RESIDENCE|  True|    True|1222|    12.0|27.0|          27.0|      02|         0.0|         0.0|2001|07/01/2023 03:40:...|      0.0|       0.0|                   0|\n",
      "| 9755214|   HX403555|2001-01-01|031XX W DOUGLAS BLVD|1562|         SEX OFFENSE|AGG CRIMINAL SEXU...|CHURCH/SYNAGOGUE/...| False|   False|1022|    10.0|24.0|          29.0|      17|         0.0|         0.0|2001|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "|10473864|   HZ213356|2001-01-01|   012XX S DAMEN AVE|1582|OFFENSE INVOLVING...|   CHILD PORNOGRAPHY|               OTHER| False|   False|1233|    12.0| 2.0|          28.0|      17|         0.0|         0.0|2001|04/09/2016 03:47:...|      0.0|       0.0|                   0|\n",
      "+--------+-----------+----------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+--------+--------------------+----------+----------+----------------+----------------+-----------------+--------+\n",
      "|Longitude|Latitude|        Station_Name|Climate_ID|      Date|Max_Temp_Celcius|Min_Temp_Celcius|Mean_Temp_Celcius|Province|\n",
      "+---------+--------+--------------------+----------+----------+----------------+----------------+-----------------+--------+\n",
      "|  -134.83|   60.47| ANNIE LAKE ROBINSON|   2100115|2000-01-01|           -25.0|           -30.0|            -27.5|      YT|\n",
      "|   -62.17|   46.43|         EAST BALTIC|   8300416|2000-01-01|            -4.0|           -12.0|             -8.0|      PE|\n",
      "|  -121.22|   51.68|  100 MILE HOUSE 6NE|   1165793|2000-01-01|            -7.5|           -10.0|             -8.8|      BC|\n",
      "|  -115.96|   49.63|       KIMBERLEY PCC|   1154203|2000-01-01|            -3.0|           -14.5|             -8.8|      BC|\n",
      "|  -123.12|   49.31| N VANCOUVER WHARVES|   1105669|2000-01-01|            NULL|            NULL|             NULL|      BC|\n",
      "|  -113.29|    49.2|            CARDSTON|   3031322|2000-01-01|             3.3|            -7.1|             -1.9|      AB|\n",
      "|   -77.48|   44.53|               MADOC|   6154779|2000-01-01|             6.0|            -8.5|             -1.3|      ON|\n",
      "|  -114.88|   50.62|         BURNS CREEK|   3050974|2000-01-01|            -5.6|           -14.3|            -10.0|      AB|\n",
      "|   -75.37|    45.0|          OAK VALLEY|   6105730|2000-01-01|             4.0|           -10.0|             -3.0|      ON|\n",
      "|   -73.85|   45.22|         STE MARTINE|   7027540|2000-01-01|             3.5|           -13.0|             -4.8|      QC|\n",
      "|  -124.32|   49.77|STILLWATER POWER ...|   1047770|2000-01-01|            NULL|            NULL|             NULL|      BC|\n",
      "|   -65.83|    47.9|        BELLEDUNE CS|   8100515|2000-01-01|            -3.4|           -21.8|            -12.6|      NB|\n",
      "|   -81.15|   43.86|            WROXETER|   6129660|2000-01-01|            NULL|            NULL|             NULL|      ON|\n",
      "|   -123.2|    49.4|        CYPRESS BOWL|   1102253|2000-01-01|             0.0|            -2.0|             -1.0|      BC|\n",
      "|  -113.89|   52.18|          RED DEER A|   3025480|2000-01-01|            -5.2|           -16.4|            -10.8|      AB|\n",
      "|  -115.22|   49.23|BAYNES LAKE KOOTE...|   1150690|2000-01-01|            -1.0|            -7.0|             -4.0|      BC|\n",
      "|   -63.35|   46.41|         NEW GLASGOW|   8300497|2000-01-01|             0.0|           -19.5|             -9.8|      PE|\n",
      "|  -104.65|   49.38|              CEYLON|   4011441|2000-01-01|            NULL|            NULL|             NULL|      SK|\n",
      "|   -64.77|   44.08|    LIVERPOOL MILTON|   8203120|2000-01-01|            NULL|           -14.0|             NULL|      NS|\n",
      "|   -64.65|   61.58|   RESOLUTION ISLAND|   2403602|2000-01-01|           -19.0|           -22.6|            -20.8|      NU|\n",
      "+---------+--------+--------------------+----------+----------+----------------+----------------+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "USA_Crimes.show()\n",
    "ca_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4d7cd94-b35c-4c05-85c0-11c34fc45ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "# Convert the 'Date' column to datetime and change the format\n",
    "USA_Crimes = USA_Crimes.withColumn(\"Date\", to_date(col(\"Date\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Sort the DataFrame by the 'Date' column\n",
    "USA_Crimes = USA_Crimes.orderBy(\"Date\", ascending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9aae5a45-358d-4f67-9b64-612323c8893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|      Date|us_crime_count|\n",
      "+----------+--------------+\n",
      "|2023-06-22|           784|\n",
      "|2022-07-31|           761|\n",
      "|2023-07-15|           787|\n",
      "|2016-03-01|           821|\n",
      "|2021-10-11|           565|\n",
      "|2014-09-26|           879|\n",
      "|2021-12-18|           577|\n",
      "|2005-06-06|          1275|\n",
      "|2013-05-21|           932|\n",
      "|2015-05-19|           710|\n",
      "|2022-03-28|           601|\n",
      "|2017-08-11|           766|\n",
      "|2021-11-13|           589|\n",
      "|2021-01-27|           518|\n",
      "|2021-08-27|           674|\n",
      "|2014-11-12|           596|\n",
      "|2009-11-22|           988|\n",
      "|2017-09-11|           691|\n",
      "|2005-01-16|           968|\n",
      "|2018-08-10|           859|\n",
      "+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by 'Date' and count the rows for each group\n",
    "df_aggregated = USA_Crimes.groupBy(\"Date\").agg(count(\"*\").alias(\"us_crime_count\"))\n",
    "\n",
    "df_aggregated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a472046-3f7f-4f0d-b907-4e75489e34e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 16:57:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "'''spark_postgres = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Canada Temperature Data Analysis\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/postgresql-42.7.3.jar\") \\\n",
    "    .getOrCreate()'''\n",
    "\n",
    "spark_postgres = SparkSession.builder \\\n",
    "    .appName(\"Canada Temperature Data Analysis\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/postgresql-42.7.3.jar\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6311cc0-a50c-40b9-9f30-c7400a92225f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "us_temp = spark_postgres.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres_db\") \\\n",
    "    .option(\"dbtable\", \"us_temperature\") \\\n",
    "    .option(\"user\", \"postgres_user\") \\\n",
    "    .option(\"password\", \"postgres_user\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a309d443-0577-451a-a3d9-8783ac4b71d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+--------+-----------+-----------+------------+-----+--------+\n",
      "|wbanno|  lst_date|longitude|latitude|t_daily_max|t_daily_min|t_daily_mean|state|location|\n",
      "+------+----------+---------+--------+-----------+-----------+------------+-----+--------+\n",
      "| 53159|2013-08-12|  -111.34|    35.5|       32.3|       14.0|        23.1|   AZ| Cameron|\n",
      "| 53159|2013-08-13|  -111.34|    35.5|       33.8|       15.1|        24.5|   AZ| Cameron|\n",
      "| 53159|2013-08-14|  -111.34|    35.5|       34.4|       16.5|        25.4|   AZ| Cameron|\n",
      "| 53159|2013-08-15|  -111.34|    35.5|       33.6|       18.8|        26.2|   AZ| Cameron|\n",
      "| 53159|2013-08-16|  -111.34|    35.5|       33.3|       15.8|        24.5|   AZ| Cameron|\n",
      "+------+----------+---------+--------+-----------+-----------+------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "us_temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3dd2003-459d-4e56-b156-7b62ab679ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a Spark MySQL Session\n",
      "**Created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 16:58:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "print(\"Create a Spark MySQL Session\")\n",
    "appName = \"PySpark Example\"\n",
    "master = \"local\"\n",
    "# Create Spark session\n",
    "spark_mysql = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/mysql-connector-java-8.0.30.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "server = \"localhost\"\n",
    "port = 3306\n",
    "database = \"project_db\"\n",
    "jdbc_url = f\"jdbc:mysql://{server}:{port}/{database}?permitMysqlScheme\"\n",
    "\n",
    "\n",
    "user = \"root\"\n",
    "password = \"AzurePassword\"\n",
    "jdbc_driver = \"com.mysql.cj.jdbc.Driver\"\n",
    "\n",
    "\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": jdbc_driver\n",
    "}\n",
    "print(\"**Created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72bca804-4627-427f-9c36-2cd966ea7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_birth = spark_mysql.read.jdbc(jdbc_url, \"(select * from us_birth_data) tab\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96129470-66b2-4d91-98ea-97f90f7d13f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      Date|us_births|\n",
      "+----------+---------+\n",
      "|1969-01-01|     8486|\n",
      "|1969-01-02|     9002|\n",
      "|1969-01-03|     9542|\n",
      "|1969-01-04|     8960|\n",
      "|1969-01-05|     8390|\n",
      "|1969-01-06|     9560|\n",
      "|1969-01-07|     9738|\n",
      "|1969-01-08|     9734|\n",
      "|1969-01-09|     9434|\n",
      "|1969-01-10|    10042|\n",
      "|1969-01-11|     9178|\n",
      "|1969-01-12|     8450|\n",
      "|1969-01-13|     9834|\n",
      "|1969-01-14|    10366|\n",
      "|1969-01-15|     9894|\n",
      "|1969-01-16|     9662|\n",
      "|1969-01-17|     9974|\n",
      "|1969-01-18|     9312|\n",
      "|1969-01-19|     8622|\n",
      "|1969-01-20|     9808|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_birth.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "018a7183-f81e-4fcd-9db8-1997580658f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrames as temporary SQL tables\n",
    "ca_temp.createOrReplaceTempView(\"CA_temperature\")\n",
    "\n",
    "\n",
    "# Run SQL queries for analysis\n",
    "ca_on_temp = spark_hadoop.sql(\"\"\"\n",
    "    SELECT Longitude, Latitude, Station_Name, Date, Max_Temp_Celcius, Min_Temp_Celcius, Mean_Temp_Celcius, Province\n",
    "    FROM ca_temperature\n",
    "    WHERE Province = \"ON\" and  Station_Name=\"COBOURG STP\"\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "51c2d889-2046-4280-b913-2f5c3c25b94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+----------+----------------+----------------+-----------------+--------+\n",
      "|Longitude|Latitude|Station_Name|      Date|Max_Temp_Celcius|Min_Temp_Celcius|Mean_Temp_Celcius|Province|\n",
      "+---------+--------+------------+----------+----------------+----------------+-----------------+--------+\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-01|             6.0|            -5.0|              0.5|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-02|             9.0|            -2.0|              3.5|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-03|             7.5|             0.5|              4.0|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-04|             9.0|            -1.0|              4.0|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-05|             0.0|            -9.0|             -4.5|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-06|             3.5|           -10.0|             -3.3|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-07|             2.5|            -1.0|              0.8|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-08|             4.5|            -5.5|             -0.5|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-09|             4.5|             2.0|              3.3|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-10|             6.0|             2.0|              4.0|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-11|             5.0|             3.0|              4.0|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-12|             1.0|            -2.5|             -0.8|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-13|            -9.0|           -10.5|             -9.8|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-14|            -9.0|           -17.5|            -13.3|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-15|             4.0|           -14.0|             -5.0|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-16|             2.5|           -13.0|             -5.3|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-17|           -13.0|           -23.0|            -18.0|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-18|           -10.0|           -17.0|            -13.5|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-19|            -4.0|           -14.0|             -9.0|      ON|\n",
      "|   -78.18|   43.97| COBOURG STP|2000-01-20|            -6.0|           -14.0|            -10.0|      ON|\n",
      "+---------+--------+------------+----------+----------------+----------------+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "ca_on_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1b4e29ef-087c-4917-9b15-c5c139530353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 198:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+--------+-----------+-----------+------------+-----+--------+\n",
      "|wbanno|  lst_date|longitude|latitude|t_daily_max|t_daily_min|t_daily_mean|state|location|\n",
      "+------+----------+---------+--------+-----------+-----------+------------+-----+--------+\n",
      "| 54795|2015-11-14|   -70.95|   43.11|        7.3|        0.9|         4.1|   NH|  Durham|\n",
      "| 54795|2015-11-15|   -70.95|   43.11|       11.8|       -0.8|         5.5|   NH|  Durham|\n",
      "| 54795|2015-11-16|   -70.95|   43.11|       13.1|       -0.9|         6.1|   NH|  Durham|\n",
      "| 54795|2015-11-17|   -70.95|   43.11|        7.5|       -5.6|         1.0|   NH|  Durham|\n",
      "| 54795|2015-11-18|   -70.95|   43.11|        9.7|       -7.8|         0.9|   NH|  Durham|\n",
      "| 54795|2015-11-19|   -70.95|   43.11|       11.1|       -2.5|         4.3|   NH|  Durham|\n",
      "| 54795|2015-11-20|   -70.95|   43.11|       14.9|        5.5|        10.2|   NH|  Durham|\n",
      "| 54795|2015-11-21|   -70.95|   43.11|        8.6|       -2.9|         2.8|   NH|  Durham|\n",
      "| 54795|2015-11-22|   -70.95|   43.11|        5.9|       -2.1|         1.9|   NH|  Durham|\n",
      "| 54795|2015-11-23|   -70.95|   43.11|        4.5|       -3.0|         0.7|   NH|  Durham|\n",
      "| 54795|2015-11-24|   -70.95|   43.11|        5.4|       -7.6|        -1.1|   NH|  Durham|\n",
      "| 54795|2015-11-25|   -70.95|   43.11|        6.8|       -7.1|        -0.1|   NH|  Durham|\n",
      "| 54795|2015-11-26|   -70.95|   43.11|       13.5|       -5.4|         4.1|   NH|  Durham|\n",
      "| 54795|2015-11-27|   -70.95|   43.11|       18.5|        1.1|         9.8|   NH|  Durham|\n",
      "| 54795|2015-11-28|   -70.95|   43.11|       13.9|        1.3|         7.6|   NH|  Durham|\n",
      "| 54795|2015-11-29|   -70.95|   43.11|        5.5|       -5.2|         0.2|   NH|  Durham|\n",
      "| 54795|2015-11-30|   -70.95|   43.11|        1.5|       -7.6|        -3.1|   NH|  Durham|\n",
      "| 54795|2015-12-01|   -70.95|   43.11|        2.4|       -7.9|        -2.8|   NH|  Durham|\n",
      "| 54795|2015-12-02|   -70.95|   43.11|        5.8|        2.0|         3.9|   NH|  Durham|\n",
      "| 54795|2015-12-03|   -70.95|   43.11|        6.6|        2.1|         4.3|   NH|  Durham|\n",
      "+------+----------+---------+--------+-----------+-----------+------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "us_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f23ca8-37e9-434f-b933-deaf60cc3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "3047\n",
    "53968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efa21533-c05d-4145-bebc-a867923dacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrames as temporary SQL tables\n",
    "us_temp.createOrReplaceTempView(\"us_temperature\")\n",
    "\n",
    "\n",
    "# Run SQL queries for analysis\n",
    "us_tx_temp = spark_postgres.sql(\"\"\"\n",
    "    SELECT lst_date, longitude, latitude, t_daily_max, t_daily_min, t_daily_mean, state\n",
    "    FROM us_temperature\n",
    "    WHERE state = \"TX\" and wbanno=\"3047\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd9df6d4-382b-4c4a-b92a-2af053766f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 7714 rows and 7 columns.\n"
     ]
    }
   ],
   "source": [
    "# Get number of rows\n",
    "num_rows = us_tx_temp.count()\n",
    "\n",
    "# Get number of columns\n",
    "num_columns = len(us_tx_temp.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(f\"The dataset has {num_rows} rows and {num_columns} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ab6e8eb-83bf-463d-a0e9-45150ea3435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-----------+-----------+------------+-----+\n",
      "|  lst_date|longitude|latitude|t_daily_max|t_daily_min|t_daily_mean|state|\n",
      "+----------+---------+--------+-----------+-----------+------------+-----+\n",
      "|2006-01-01|  -102.81|   31.62|       23.8|       10.3|        17.1|   TX|\n",
      "|2006-05-16|  -102.81|   31.62|       29.5|       10.2|        19.9|   TX|\n",
      "|2006-05-17|  -102.81|   31.62|       32.7|       12.7|        22.7|   TX|\n",
      "|2006-05-18|  -102.81|   31.62|       36.3|       14.8|        25.6|   TX|\n",
      "|2006-05-19|  -102.81|   31.62|       38.4|       16.6|        27.5|   TX|\n",
      "|2006-05-20|  -102.81|   31.62|       39.9|       18.7|        29.3|   TX|\n",
      "|2006-05-21|  -102.81|   31.62|       38.8|       21.6|        30.2|   TX|\n",
      "|2006-05-22|  -102.81|   31.62|       35.5|       20.5|        28.0|   TX|\n",
      "|2006-05-23|  -102.81|   31.62|       39.1|       21.8|        30.4|   TX|\n",
      "|2006-05-24|  -102.81|   31.62|       39.0|       21.3|        30.1|   TX|\n",
      "|2006-05-25|  -102.81|   31.62|       38.4|       19.5|        29.0|   TX|\n",
      "|2006-05-26|  -102.81|   31.62|       38.4|       23.0|        30.7|   TX|\n",
      "|2006-05-27|  -102.81|   31.62|       36.2|       20.0|        28.1|   TX|\n",
      "|2006-05-28|  -102.81|   31.62|       37.3|       20.6|        29.0|   TX|\n",
      "|2006-05-29|  -102.81|   31.62|       37.7|       18.9|        28.3|   TX|\n",
      "|2006-05-30|  -102.81|   31.62|       36.3|       21.8|        29.1|   TX|\n",
      "|2006-05-31|  -102.81|   31.62|       33.2|       18.9|        26.1|   TX|\n",
      "|2006-06-01|  -102.81|   31.62|       28.0|       16.4|        22.2|   TX|\n",
      "|2006-06-02|  -102.81|   31.62|       32.6|       15.8|        24.2|   TX|\n",
      "|2006-06-03|  -102.81|   31.62|       34.7|       17.0|        25.9|   TX|\n",
      "+----------+---------+--------+-----------+-----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "us_tx_temp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a13a86-6727-48f1-9cf4-79a1f00ec013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/11 16:33:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_local = SparkSession.builder \\\n",
    "    .appName(\"CSV Loading from Virtual Machine\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5402dc4d-ad87-46a0-b5fa-4aefafe67ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------+----------+----------+----------------+----------------+-----------------+--------+\n",
      "|Longitude|Latitude|       Station_Name|Climate_ID|      Date|Max_Temp_Celcius|Min_Temp_Celcius|Mean_Temp_Celcius|Province|\n",
      "+---------+--------+-------------------+----------+----------+----------------+----------------+-----------------+--------+\n",
      "|  -134.83|   60.47|ANNIE LAKE ROBINSON|   2100115|2000-01-01|           -25.0|           -30.0|            -27.5|      YT|\n",
      "|   -62.17|   46.43|        EAST BALTIC|   8300416|2000-01-01|            -4.0|           -12.0|             -8.0|      PE|\n",
      "|  -121.22|   51.68| 100 MILE HOUSE 6NE|   1165793|2000-01-01|            -7.5|           -10.0|             -8.8|      BC|\n",
      "|  -115.96|   49.63|      KIMBERLEY PCC|   1154203|2000-01-01|            -3.0|           -14.5|             -8.8|      BC|\n",
      "|  -123.12|   49.31|N VANCOUVER WHARVES|   1105669|2000-01-01|             0.0|             0.0|              0.0|      BC|\n",
      "+---------+--------+-------------------+----------+----------+----------------+----------------+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ca_temp = spark_local.read.csv('/home/azureuser/data/projectdata/CA_temperature.csv', header=True, inferSchema=True)\n",
    "\n",
    "ca_temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e8c1091-5eff-4e3c-af32-5c2ad88e0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrames as temporary SQL tables\n",
    "ca_temp.createOrReplaceTempView(\"CA_temperature\")\n",
    "\n",
    "\n",
    "# Run SQL queries for analysis\n",
    "ca_on_temp = spark_local.sql(\"\"\"\n",
    "    SELECT Longitude, Latitude, Station_Name, Date, Max_Temp_Celcius, Min_Temp_Celcius, Mean_Temp_Celcius, Province\n",
    "    FROM ca_temperature\n",
    "    WHERE Province = \"ON\" and  Station_Name=\"COBOURG STP\"\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fa05651-1e65-4ee8-b8f0-cd11a4407591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 8003 rows and 8 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get number of rows\n",
    "num_rows = ca_on_temp.count()\n",
    "\n",
    "# Get number of columns\n",
    "num_columns = len(ca_on_temp.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(f\"The dataset has {num_rows} rows and {num_columns} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b50179cc-5559-4632-bd5d-f30e03f4fdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+\n",
      "|      Date|ca_longitude|ca_latitude|ca_max_temp|ca_min_temp|ca_mean_temp|ca_province|us_longitude|us_latitude|us_max_temp|us_min_temp|us_mean_temp|us_state|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+\n",
      "|2000-01-06|      -78.18|      43.97|        3.5|      -10.0|        -3.3|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|\n",
      "|2000-01-05|      -78.18|      43.97|        0.0|       -9.0|        -4.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|\n",
      "|2000-01-02|      -78.18|      43.97|        9.0|       -2.0|         3.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|\n",
      "|2000-01-01|      -78.18|      43.97|        6.0|       -5.0|         0.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|\n",
      "|2000-01-04|      -78.18|      43.97|        9.0|       -1.0|         4.0|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the join and select the required columns\n",
    "join1 = ca_on_temp.join(\n",
    "    us_ny_temp, ca_on_temp.Date == us_ny_temp.lst_date, \"left_outer\"\n",
    ").select(\n",
    "    ca_on_temp[\"Date\"],\n",
    "    ca_on_temp[\"Longitude\"].alias(\"ca_longitude\"),\n",
    "    ca_on_temp[\"Latitude\"].alias(\"ca_latitude\"),\n",
    "    ca_on_temp[\"Max_Temp_Celcius\"].alias(\"ca_max_temp\"),\n",
    "    ca_on_temp[\"Min_Temp_Celcius\"].alias(\"ca_min_temp\"),\n",
    "    ca_on_temp[\"Mean_Temp_Celcius\"].alias(\"ca_mean_temp\"),\n",
    "    ca_on_temp[\"Province\"].alias(\"ca_province\"),\n",
    "    us_ny_temp[\"LONGITUDE\"].alias(\"us_longitude\"),\n",
    "    us_ny_temp[\"LATITUDE\"].alias(\"us_latitude\"),\n",
    "    us_ny_temp[\"T_DAILY_MAX\"].alias(\"us_max_temp\"),\n",
    "    us_ny_temp[\"T_DAILY_MIN\"].alias(\"us_min_temp\"),\n",
    "    us_ny_temp[\"T_DAILY_MEAN\"].alias(\"us_mean_temp\"),\n",
    "    us_ny_temp[\"STATE\"].alias(\"us_state\")\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "join1.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5d986f5-05e5-42be-b788-e516aad291c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 8003 rows and 13 columns.\n"
     ]
    }
   ],
   "source": [
    "# Get number of rows\n",
    "num_rows = join1.count()\n",
    "\n",
    "# Get number of columns\n",
    "num_columns = len(join1.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(f\"The dataset has {num_rows} rows and {num_columns} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01f2536a-f35c-4944-9215-2196b324d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+---------+----------+--------------------+\n",
      "|      ID|Case_Number|                Date|               Block|IUCR|Primary_Type|         Description|Location_Description|Arrest|Domestic|Beat|District|Ward|Community_Area|FBI_Code|X_Coordinate|Y_Coordinate|Year|          Updated_On| Latitude| Longitude|            Location|\n",
      "+--------+-----------+--------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+---------+----------+--------------------+\n",
      "| 5741943|   HN549294|08/25/2007 09:22:...|  074XX N ROGERS AVE|0560|     ASSAULT|              SIMPLE|               OTHER| false|   false|2422|    24.0|49.0|           1.0|     08A|         0.0|         0.0|2007|08/17/2015 03:03:...|      0.0|       0.0|                   0|\n",
      "|   25953|   JE240540|05/24/2021 03:06:...| 020XX N LARAMIE AVE|0110|    HOMICIDE| FIRST DEGREE MURDER|              STREET|  true|   false|2515|    25.0|36.0|          19.0|     01A|   1141387.0|   1913179.0|2021|11/18/2023 03:39:...| 41.91784| -87.75597|(41.917838056, -8...|\n",
      "|   26038|   JE279849|06/26/2021 09:24:...|062XX N MC CORMIC...|0110|    HOMICIDE| FIRST DEGREE MURDER|         PARKING LOT|  true|   false|1711|    17.0|50.0|          13.0|     01A|   1152781.0|   1941458.0|2021|11/18/2023 03:39:...| 41.99522|-87.713356|(41.995219444, -8...|\n",
      "|13279676|   JG507211|11/09/2023 07:30:...|    019XX W BYRON ST|0620|    BURGLARY|      UNLAWFUL ENTRY|           APARTMENT| false|   false|1922|    19.0|47.0|           5.0|      05|   1162518.0|   1925906.0|2023|11/18/2023 03:39:...|41.952347| -87.67798|(41.952345086, -8...|\n",
      "|13274752|   JG501049|11/12/2023 07:59:...|086XX S COTTAGE G...|0454|     BATTERY|AGGRAVATED P.O. -...|  SMALL RETAIL STORE|  true|   false| 632|     6.0| 6.0|          44.0|     08B|   1183071.0|   1847869.0|2023|12/09/2023 03:41:...| 41.73775| -87.60486|(41.737750767, -8...|\n",
      "+--------+-----------+--------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usa_crimes = spark_local.read.csv('/home/azureuser/data/projectdata/USA_Crimes_-_2001_to_Present.csv', header=True, inferSchema=True)\n",
    "usa_crimes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "144d5409-1010-42b0-baa2-479da01b419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|      Date|us_crime_count|\n",
      "+----------+--------------+\n",
      "|2023-06-22|           784|\n",
      "|2022-07-31|           761|\n",
      "|2023-07-15|           787|\n",
      "|2016-03-01|           821|\n",
      "|2021-10-11|           565|\n",
      "|2014-09-26|           879|\n",
      "|2021-12-18|           577|\n",
      "|2005-06-06|          1275|\n",
      "|2013-05-21|           932|\n",
      "|2015-05-19|           710|\n",
      "|2022-03-28|           601|\n",
      "|2017-08-11|           766|\n",
      "|2021-11-13|           589|\n",
      "|2021-01-27|           518|\n",
      "|2021-08-27|           674|\n",
      "|2014-11-12|           596|\n",
      "|2009-11-22|           988|\n",
      "|2017-09-11|           691|\n",
      "|2005-01-16|           968|\n",
      "|2018-08-10|           859|\n",
      "+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "# Convert the 'Date' column to datetime and change the format\n",
    "usa_crimes = usa_crimes.withColumn(\"Date\", to_date(col(\"Date\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Sort the DataFrame by the 'Date' column\n",
    "usa_crimes = usa_crimes.orderBy(\"Date\", ascending=True)\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by 'Date' and count the rows for each group\n",
    "df_aggregated = usa_crimes.groupBy(\"Date\").agg(count(\"*\").alias(\"us_crime_count\"))\n",
    "\n",
    "df_aggregated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40e240e1-18bb-4fe9-a3b8-97837a61aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+\n",
      "|      Date|ca_longitude|ca_latitude|ca_max_temp|ca_min_temp|ca_mean_temp|ca_province|us_longitude|us_latitude|us_max_temp|us_min_temp|us_mean_temp|us_state|us_crime_count|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+\n",
      "|2000-01-06|      -78.18|      43.97|        3.5|      -10.0|        -3.3|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|\n",
      "|2000-01-05|      -78.18|      43.97|        0.0|       -9.0|        -4.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|\n",
      "|2000-01-02|      -78.18|      43.97|        9.0|       -2.0|         3.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|\n",
      "|2000-01-01|      -78.18|      43.97|        6.0|       -5.0|         0.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|\n",
      "|2000-01-04|      -78.18|      43.97|        9.0|       -1.0|         4.0|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Perform the join and select the required columns\n",
    "join2 = join1.join(\n",
    "    df_aggregated, join1.Date == df_aggregated.Date, \"left_outer\"\n",
    ").select(\n",
    "    join1[\"*\"],\n",
    "    df_aggregated[\"us_crime_count\"]\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "join2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b08b3e7-b649-4a94-8952-db47674d273a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 8003 rows and 14 columns.\n"
     ]
    }
   ],
   "source": [
    "# Get number of rows\n",
    "num_rows = join2.count()\n",
    "\n",
    "# Get number of columns\n",
    "num_columns = len(join2.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(f\"The dataset has {num_rows} rows and {num_columns} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e851fab-028c-4724-8089-38b488e10fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:===================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+\n",
      "|      Date|ca_longitude|ca_latitude|ca_max_temp|ca_min_temp|ca_mean_temp|ca_province|us_longitude|us_latitude|us_max_temp|us_min_temp|us_mean_temp|us_state|us_crime_count|us_births|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+\n",
      "|2000-01-06|      -78.18|      43.97|        3.5|      -10.0|        -3.3|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|\n",
      "|2000-01-05|      -78.18|      43.97|        0.0|       -9.0|        -4.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|\n",
      "|2000-01-02|      -78.18|      43.97|        9.0|       -2.0|         3.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|\n",
      "|2000-01-01|      -78.18|      43.97|        6.0|       -5.0|         0.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|\n",
      "|2000-01-04|      -78.18|      43.97|        9.0|       -1.0|         4.0|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Perform the join and select the required columns\n",
    "join3 = join2.join(\n",
    "    us_birth, join2.Date == us_birth.Date, \"left_outer\"\n",
    ").select(\n",
    "    join2[\"*\"],\n",
    "    us_birth[\"us_births\"]\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "join3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e048a7b1-3b39-43e7-8113-b9d07a163375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 118:===================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 8003 rows and 15 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get number of rows\n",
    "num_rows = join3.count()\n",
    "\n",
    "# Get number of columns\n",
    "num_columns = len(join3.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(f\"The dataset has {num_rows} rows and {num_columns} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b5b5e38-4380-4055-9b87-80406b8c668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2021, 4, 12), ca_longitude=-78.18, ca_latitude=43.97, ca_max_temp=10.0, ca_min_temp=7.0, ca_mean_temp=8.5, ca_province='ON', us_longitude=-102.81, us_latitude=31.62, us_max_temp=26.6, us_min_temp=11.5, us_mean_temp=19.1, us_state='TX', us_crime_count=533, us_births=9690),\n",
       " Row(Date=datetime.date(2021, 5, 13), ca_longitude=-78.18, ca_latitude=43.97, ca_max_temp=17.0, ca_min_temp=8.5, ca_mean_temp=12.8, ca_province='ON', us_longitude=-102.81, us_latitude=31.62, us_max_temp=26.9, us_min_temp=12.1, us_mean_temp=19.5, us_state='TX', us_crime_count=547, us_births=10372),\n",
       " Row(Date=datetime.date(2022, 8, 31), ca_longitude=-78.18, ca_latitude=43.97, ca_max_temp=24.0, ca_min_temp=14.0, ca_mean_temp=19.0, ca_province='ON', us_longitude=-102.81, us_latitude=31.62, us_max_temp=26.4, us_min_temp=20.2, us_mean_temp=23.3, us_state='TX', us_crime_count=739, us_births=9530),\n",
       " Row(Date=datetime.date(2022, 9, 13), ca_longitude=-78.18, ca_latitude=43.97, ca_max_temp=23.0, ca_min_temp=15.5, ca_mean_temp=19.3, ca_province='ON', us_longitude=-102.81, us_latitude=31.62, us_max_temp=33.2, us_min_temp=18.4, us_mean_temp=25.8, us_state='TX', us_crime_count=678, us_births=10044),\n",
       " Row(Date=datetime.date(2023, 6, 9), ca_longitude=-78.18, ca_latitude=43.97, ca_max_temp=0.0, ca_min_temp=0.0, ca_mean_temp=0.0, ca_province='ON', us_longitude=-102.81, us_latitude=31.62, us_max_temp=37.6, us_min_temp=20.1, us_mean_temp=28.9, us_state='TX', us_crime_count=699, us_births=10405)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join3.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "734e5b9e-4cce-4d93-b242-f2648b0083b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+--------+---------------+--------------------+--------------------+-----------+------------+----------+----------+----------+-----------+--------+---------+-------+-------+----------+--------+--------+--------------------+-------------+--------+-------+-------------------+------------+--------+--------------------+--------+--------------------+-----------------+----------------+\n",
      "|                X|               Y|OBJECTID|EVENT_UNIQUE_ID|         REPORT_DATE|            OCC_DATE|REPORT_YEAR|REPORT_MONTH|REPORT_DAY|REPORT_DOY|REPORT_DOW|REPORT_HOUR|OCC_YEAR|OCC_MONTH|OCC_DAY|OCC_DOY|   OCC_DOW|OCC_HOUR|DIVISION|       LOCATION_TYPE|PREMISES_TYPE|UCR_CODE|UCR_EXT|            OFFENCE|MCI_CATEGORY|HOOD_158|   NEIGHBOURHOOD_158|HOOD_140|   NEIGHBOURHOOD_140|       LONG_WGS84|       LAT_WGS84|\n",
      "+-----------------+----------------+--------+---------------+--------------------+--------------------+-----------+------------+----------+----------+----------+-----------+--------+---------+-------+-------+----------+--------+--------+--------------------+-------------+--------+-------+-------------------+------------+--------+--------------------+--------+--------------------+-----------------+----------------+\n",
      "|-8841784.90871943|5410286.47089473|       1| GO-20141260127|2014/01/01 05:00:...|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |          1|    2014|  January|      1|      1|Wednesday |       1|     D14|    Bar / Restaurant|   Commercial|    1420|    110|Assault Bodily Harm|     Assault|      84|     Little Portugal|      84|Little Portugal (84)| -79.427105224297| 43.642516787878|\n",
      "|-8838125.64948372|5412177.83243115|       2| GO-20141263725|2014/01/01 05:00:...|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         19|    2014|  January|      1|      1|Wednesday |      18|     D52|Apartment (Roomin...|    Apartment|    1420|    100|Assault With Weapon|     Assault|      78|Kensington-Chinatown|      78|Kensington-Chinat...|-79.3942335387789|43.6548107944083|\n",
      "|-8841053.22819299|5409638.73626077|       3| GO-20141264272|2014/01/01 05:00:...|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         21|    2014|  January|      1|      1|Wednesday |      21|     D14|Apartment (Roomin...|    Apartment|    1420|    100|Assault With Weapon|     Assault|      85|      South Parkdale|      85| South Parkdale (85)|-79.4205324264879| 43.638305880956|\n",
      "|-8850460.85149563|5420267.41484981|       4| GO-20141263706|2014/01/01 05:00:...|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         18|    2014|  January|      1|      1|Wednesday |      18|     D12|Single Home, Hous...|        House|    1430|    100|            Assault|     Assault|     113|              Weston|     113|        Weston (113)|-79.5050425436361|43.7073653479291|\n",
      "|-8840469.99476786|5431265.81442916|       5| GO-20141263710|2014/01/01 05:00:...|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         18|    2014|  January|      1|      1|Wednesday |      18|     D32|Apartment (Roomin...|    Apartment|    1430|    100|            Assault|     Assault|     151|         Yonge-Doris|      51|Willowdale East (51)|-79.4152931507629|43.7787433961406|\n",
      "+-----------------+----------------+--------+---------------+--------------------+--------------------+-----------+------------+----------+----------+----------+-----------+--------+---------+-------+-------+----------+--------+--------+--------------------+-------------+--------+-------+-------------------+------------+--------+--------------------+--------+--------------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ca_crim = spark_local.read.csv('/home/azureuser/data/projectdata/Canada_Major_Crime_Indicators_Open_Data.csv', header=True, inferSchema=True)\n",
    "\n",
    "ca_crim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e67f5f5c-8e6d-4914-a3a5-64d78806d254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 145:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+--------+---------------+-----------+--------------------+-----------+------------+----------+----------+----------+-----------+--------+---------+-------+-------+----------+--------+--------+--------------------+-------------+--------+-------+--------------------+---------------+--------+--------------------+--------+--------------------+-----------------+----------------+\n",
      "|                X|               Y|OBJECTID|EVENT_UNIQUE_ID|REPORT_DATE|            OCC_DATE|REPORT_YEAR|REPORT_MONTH|REPORT_DAY|REPORT_DOY|REPORT_DOW|REPORT_HOUR|OCC_YEAR|OCC_MONTH|OCC_DAY|OCC_DOY|   OCC_DOW|OCC_HOUR|DIVISION|       LOCATION_TYPE|PREMISES_TYPE|UCR_CODE|UCR_EXT|             OFFENCE|   MCI_CATEGORY|HOOD_158|   NEIGHBOURHOOD_158|HOOD_140|   NEIGHBOURHOOD_140|       LONG_WGS84|       LAT_WGS84|\n",
      "+-----------------+----------------+--------+---------------+-----------+--------------------+-----------+------------+----------+----------+----------+-----------+--------+---------+-------+-------+----------+--------+--------+--------------------+-------------+--------+-------+--------------------+---------------+--------+--------------------+--------+--------------------+-----------------+----------------+\n",
      "|-8841784.90871943|5410286.47089473|       1| GO-20141260127| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |          1|    2014|  January|      1|      1|Wednesday |       1|     D14|    Bar / Restaurant|   Commercial|    1420|    110| Assault Bodily Harm|        Assault|      84|     Little Portugal|      84|Little Portugal (84)| -79.427105224297| 43.642516787878|\n",
      "|-8838125.64948372|5412177.83243115|       2| GO-20141263725| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         19|    2014|  January|      1|      1|Wednesday |      18|     D52|Apartment (Roomin...|    Apartment|    1420|    100| Assault With Weapon|        Assault|      78|Kensington-Chinatown|      78|Kensington-Chinat...|-79.3942335387789|43.6548107944083|\n",
      "|-8841053.22819299|5409638.73626077|       3| GO-20141264272| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         21|    2014|  January|      1|      1|Wednesday |      21|     D14|Apartment (Roomin...|    Apartment|    1420|    100| Assault With Weapon|        Assault|      85|      South Parkdale|      85| South Parkdale (85)|-79.4205324264879| 43.638305880956|\n",
      "|-8850460.85149563|5420267.41484981|       4| GO-20141263706| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         18|    2014|  January|      1|      1|Wednesday |      18|     D12|Single Home, Hous...|        House|    1430|    100|             Assault|        Assault|     113|              Weston|     113|        Weston (113)|-79.5050425436361|43.7073653479291|\n",
      "|-8840469.99476786|5431265.81442916|       5| GO-20141263710| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         18|    2014|  January|      1|      1|Wednesday |      18|     D32|Apartment (Roomin...|    Apartment|    1430|    100|             Assault|        Assault|     151|         Yonge-Doris|      51|Willowdale East (51)|-79.4152931507629|43.7787433961406|\n",
      "| -8827704.4102312| 5418273.1925597|       6| GO-20141263744| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         18|    2014|  January|      1|      1|Wednesday |      18|     D54|Apartment (Roomin...|    Apartment|    1420|    100| Assault With Weapon|        Assault|      61|       Taylor-Massey|      61|  Taylor-Massey (61)|-79.3006179542788|43.6944140148806|\n",
      "|-8829963.71877897|5428278.12269902|       7| GO-20141263357| 2014-01-01|2013/12/29 05:00:...|       2014|     January|         1|         1|Wednesday |         17|    2013| December|     29|    363|Sunday    |      12|     D33|Apartment (Roomin...|    Apartment|    1430|    100|             Assault|        Assault|     150|   Fenside-Parkwoods|      45|Parkwoods-Donalda...|-79.3209136681278|43.7593621211978|\n",
      "|-8838776.84668624|5419974.99216744|       8| GO-20141264140| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         20|    2014|  January|      1|      1|Wednesday |      20|     D53|Single Home, Hous...|        House|    1430|    100|             Assault|        Assault|     100|      Yonge-Eglinton|     100|Yonge-Eglinton (100)|-79.4000833426357|43.7054664049601|\n",
      "|-8832824.78041895|5419631.29079007|       9| GO-20141259834| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |          0|    2014|  January|      1|      1|Wednesday |       0|     D53|    Bar / Restaurant|   Commercial|    1420|    100| Assault With Weapon|        Assault|      55|    Thorncliffe Park|      55|Thorncliffe Park ...|-79.3466150216221|43.7032343898983|\n",
      "|-8841257.90098799|5415106.39398807|      10| GO-20141262553| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         14|    2014|  January|      1|      1|Wednesday |      13|     D13|     Retirement Home|        Other|    1430|    100|             Assault|        Assault|      94|            Wychwood|      94|       Wychwood (94)|-79.4223710327978|43.6738417211567|\n",
      "|-8838988.35260555|5411068.10619421|      11| GO-20141260912| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |          6|    2014|  January|      1|      1|Wednesday |       4|     D14|Streets, Roads, H...|      Outside|    1450|    120|Discharge Firearm...|        Assault|     164|    Wellington Place|      78|Kensington-Chinat...|-79.4019833326708|43.6475977873627|\n",
      "|-8845308.52671409|5414299.53546384|      12| GO-20141264227| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         20|    2014|  January|      1|      1|Wednesday |      20|     D11|Single Home, Hous...|        House|    1430|    100|             Assault|        Assault|      91|  Weston-Pelham Park|      91|Weston-Pellam Par...|-79.4587584231452|43.6685990435469|\n",
      "|-8809035.84152658|5431523.16891597|      13| GO-20141260264| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |          1|    2014|  January|      1|      1|Wednesday |       1|     D43|            Go Train|      Transit|    1430|    100|             Assault|        Assault|     143|          West Rouge|     131|         Rouge (131)|-79.1329153478869|43.7804125709689|\n",
      "|-8826157.86123603|5419148.65896319|      14| GO-20141637937| 2014-01-01|2012/03/01 05:00:...|       2014|     January|         1|         1|Wednesday |         16|    2012|    March|      1|     61|Thursday  |       0|     D41|Apartment (Roomin...|    Apartment|    1420|    110| Assault Bodily Harm|        Assault|     120| Clairlea-Birchmount|     120|Clairlea-Birchmou...|-79.2867250682331|43.7001000127995|\n",
      "|-8840628.61134994|5412224.63766894|      15| GO-20141260618| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |          5|    2014|  January|      1|      1|Wednesday |       2|     D14|    Bar / Restaurant|   Commercial|    1430|    100|             Assault|        Assault|      81|   Trinity-Bellwoods|      81|Trinity-Bellwoods...|-79.4167180277593|43.6551149997848|\n",
      "|-8813612.76020167|5428591.47089591|      16| GO-20141260831| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |          3|    2014|  January|      1|      1|Wednesday |       3|     D43|Single Home, Hous...|        House|    1430|    100|             Assault|        Assault|     136|           West Hill|     136|     West Hill (136)| -79.174030508219|43.7613951184015|\n",
      "| -8834672.2538276|5412341.71026858|      17| GO-20141262013| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         11|    2014|  January|      1|      1|Wednesday |      11|     D51|Streets, Roads, H...|      Outside|    1420|    110| Assault Bodily Harm|        Assault|      72|         Regent Park|      72|    Regent Park (72)|-79.3632111580376|43.6558758949796|\n",
      "|-8838776.84668624|5419974.99216744|      18| GO-20141264140| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         20|    2014|  January|      1|      1|Wednesday |      20|     D53|Single Home, Hous...|        House|    1430|    100|             Assault|        Assault|     100|      Yonge-Eglinton|     100|Yonge-Eglinton (100)|-79.4000833426357|43.7054664049601|\n",
      "|-8840392.43992578| 5430179.1968087|      19| GO-20141263143| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |         16|    2014|  January|      1|      1|Wednesday |      16|     D32|Apartment (Roomin...|    Apartment|    1430|    100|             Assault|        Assault|      37|     Willowdale West|      37|Willowdale West (37)|-79.4145964643991|43.7716951914715|\n",
      "|-8840192.23905736|5411580.42895367|      20| GO-20141260521| 2014-01-01|2014/01/01 05:00:...|       2014|     January|         1|         1|Wednesday |          2|    2014|  January|      1|      1|Wednesday |       2|     D14|    Bar / Restaurant|   Commercial|    2120|    200|                 B&E|Break and Enter|      81|   Trinity-Bellwoods|      81|Trinity-Bellwoods...|-79.4127980292092|43.6509278942978|\n",
      "+-----------------+----------------+--------+---------------+-----------+--------------------+-----------+------------+----------+----------+----------+-----------+--------+---------+-------+-------+----------+--------+--------+--------------------+-------------+--------+-------+--------------------+---------------+--------+--------------------+--------+--------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import to_date, col\n",
    "# Convert REPORT_DATE to a new format\n",
    "ca_crim = ca_crim.withColumn(\"REPORT_DATE\", to_date(col(\"REPORT_DATE\").substr(1, 10), \"yyyy/MM/dd\"))\n",
    "\n",
    "# Sort the DataFrame by REPORT_DATE in ascending order\n",
    "ca_crim = ca_crim.orderBy(col(\"REPORT_DATE\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "ca_crim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a3536dd-150c-403c-94c3-022d9040b7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(X=-8816058.49316295, Y=5427704.67012902, OBJECTID=384683, EVENT_UNIQUE_ID='GO-2024688981', REPORT_DATE=datetime.date(2024, 3, 31), OCC_DATE='2024/03/30 04:00:00+00', REPORT_YEAR=2024, REPORT_MONTH='March', REPORT_DAY=31, REPORT_DOY=91, REPORT_DOW='Sunday    ', REPORT_HOUR=11, OCC_YEAR=2024, OCC_MONTH='March', OCC_DAY=30, OCC_DOY=90, OCC_DOW='Saturday  ', OCC_HOUR=16, DIVISION='D43', LOCATION_TYPE='Parking Lots (Apt., Commercial Or Non-Commercial)', PREMISES_TYPE='Outside', UCR_CODE=2135, UCR_EXT=210, OFFENCE='Theft Of Motor Vehicle', MCI_CATEGORY='Auto Theft', HOOD_158='136', NEIGHBOURHOOD_158='West Hill', HOOD_140='136', NEIGHBOURHOOD_140='West Hill (136)', LONG_WGS84=-79.1960009011566, LAT_WGS84=43.7556413926696),\n",
       " Row(X=-8853213.07796189, Y=5403034.34945701, OBJECTID=384684, EVENT_UNIQUE_ID='GO-2024690900', REPORT_DATE=datetime.date(2024, 3, 31), OCC_DATE='2024/03/31 04:00:00+00', REPORT_YEAR=2024, REPORT_MONTH='March', REPORT_DAY=31, REPORT_DOY=91, REPORT_DOW='Sunday    ', REPORT_HOUR=16, OCC_YEAR=2024, OCC_MONTH='March', OCC_DAY=31, OCC_DOY=91, OCC_DOW='Sunday    ', OCC_HOUR=16, DIVISION='D22', LOCATION_TYPE='Apartment (Rooming House, Condo)', PREMISES_TYPE='Apartment', UCR_CODE=1430, UCR_EXT=100, OFFENCE='Assault', MCI_CATEGORY='Assault', HOOD_158='19', NEIGHBOURHOOD_158='Long Branch', HOOD_140='19', NEIGHBOURHOOD_140='Long Branch (19)', LONG_WGS84=-79.5297662151173, LAT_WGS84=43.5953540789664),\n",
       " Row(X=-8837819.06759107, Y=5417384.89779554, OBJECTID=384685, EVENT_UNIQUE_ID='GO-2024690985', REPORT_DATE=datetime.date(2024, 3, 31), OCC_DATE='2024/03/31 04:00:00+00', REPORT_YEAR=2024, REPORT_MONTH='March', REPORT_DAY=31, REPORT_DOY=91, REPORT_DOW='Sunday    ', REPORT_HOUR=16, OCC_YEAR=2024, OCC_MONTH='March', OCC_DAY=31, OCC_DOY=91, OCC_DOW='Sunday    ', OCC_HOUR=16, DIVISION='D53', LOCATION_TYPE='Schools During Un-Supervised Activity', PREMISES_TYPE='Educational', UCR_CODE=1430, UCR_EXT=100, OFFENCE='Assault', MCI_CATEGORY='Assault', HOOD_158='98', NEIGHBOURHOOD_158='Rosedale-Moore Park', HOOD_140='98', NEIGHBOURHOOD_140='Rosedale-Moore Park (98)', LONG_WGS84=-79.3914794666364, LAT_WGS84=43.6886441481969),\n",
       " Row(X=-8835715.27548889, Y=5416721.55108982, OBJECTID=384686, EVENT_UNIQUE_ID='GO-2024690995', REPORT_DATE=datetime.date(2024, 3, 31), OCC_DATE='2024/03/30 04:00:00+00', REPORT_YEAR=2024, REPORT_MONTH='March', REPORT_DAY=31, REPORT_DOY=91, REPORT_DOW='Sunday    ', REPORT_HOUR=16, OCC_YEAR=2024, OCC_MONTH='March', OCC_DAY=30, OCC_DOY=90, OCC_DOW='Saturday  ', OCC_HOUR=23, DIVISION='D53', LOCATION_TYPE='Single Home, House (Attach Garage, Cottage, Mobile)', PREMISES_TYPE='House', UCR_CODE=2135, UCR_EXT=210, OFFENCE='Theft Of Motor Vehicle', MCI_CATEGORY='Auto Theft', HOOD_158='98', NEIGHBOURHOOD_158='Rosedale-Moore Park', HOOD_140='98', NEIGHBOURHOOD_140='Rosedale-Moore Park (98)', LONG_WGS84=-79.3725807805127, LAT_WGS84=43.6843350560635),\n",
       " Row(X=-8811261.90684613, Y=5431635.7548109, OBJECTID=384687, EVENT_UNIQUE_ID='GO-2024688279', REPORT_DATE=datetime.date(2024, 3, 31), OCC_DATE='2024/03/30 04:00:00+00', REPORT_YEAR=2024, REPORT_MONTH='March', REPORT_DAY=31, REPORT_DOY=91, REPORT_DOW='Sunday    ', REPORT_HOUR=9, OCC_YEAR=2024, OCC_MONTH='March', OCC_DAY=30, OCC_DOY=90, OCC_DOW='Saturday  ', OCC_HOUR=19, DIVISION='D43', LOCATION_TYPE='Single Home, House (Attach Garage, Cottage, Mobile)', PREMISES_TYPE='House', UCR_CODE=2135, UCR_EXT=210, OFFENCE='Theft Of Motor Vehicle', MCI_CATEGORY='Auto Theft', HOOD_158='133', NEIGHBOURHOOD_158='Centennial Scarborough', HOOD_140='133', NEIGHBOURHOOD_140='Centennial Scarborough (133)', LONG_WGS84=-79.1529124328744, LAT_WGS84=43.7811427765735)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_crim.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2615fef-8ad6-4f20-a09d-e02b3a1fac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|REPORT_DATE|ca_crime_count|\n",
      "+-----------+--------------+\n",
      "| 2014-09-26|           100|\n",
      "| 2014-11-12|           118|\n",
      "| 2015-03-09|            89|\n",
      "| 2015-05-19|           108|\n",
      "| 2015-03-06|            75|\n",
      "| 2014-08-01|            74|\n",
      "| 2015-04-09|            97|\n",
      "| 2014-06-03|            93|\n",
      "| 2014-01-24|            80|\n",
      "| 2014-02-16|            74|\n",
      "| 2014-06-11|           101|\n",
      "| 2015-05-10|            94|\n",
      "| 2014-02-18|            89|\n",
      "| 2014-08-13|            99|\n",
      "| 2015-03-12|            88|\n",
      "| 2015-03-16|            92|\n",
      "| 2015-04-01|            81|\n",
      "| 2015-04-24|           103|\n",
      "| 2014-07-04|            89|\n",
      "| 2014-07-06|            78|\n",
      "+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Group by 'REPORT_DATE' and count each group\n",
    "#ca_crim = ca_crim.groupBy(\"REPORT_DATE\").count().withColumnRenamed(\"count\", \"ca_crime_count\")\n",
    "\n",
    "# Group by 'Date' and count the rows for each group\n",
    "df_aggregated_ca = ca_crim.groupBy(\"REPORT_DATE\").agg(count(\"*\").alias(\"ca_crime_count\"))\n",
    "\n",
    "df_aggregated_ca.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8df478f7-27a1-4e74-aa28-a4d58f08ae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+--------------+\n",
      "|      Date|ca_longitude|ca_latitude|ca_max_temp|ca_min_temp|ca_mean_temp|ca_province|us_longitude|us_latitude|us_max_temp|us_min_temp|us_mean_temp|us_state|us_crime_count|us_births|ca_crime_count|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+--------------+\n",
      "|2000-01-06|      -78.18|      43.97|        3.5|      -10.0|        -3.3|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|\n",
      "|2000-01-05|      -78.18|      43.97|        0.0|       -9.0|        -4.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|\n",
      "|2000-01-02|      -78.18|      43.97|        9.0|       -2.0|         3.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|\n",
      "|2000-01-01|      -78.18|      43.97|        6.0|       -5.0|         0.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|\n",
      "|2000-01-04|      -78.18|      43.97|        9.0|       -1.0|         4.0|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the join and select the required columns\n",
    "join4 = join3.join(\n",
    "    df_aggregated_ca, join3.Date == df_aggregated_ca.REPORT_DATE, \"left_outer\"\n",
    ").select(\n",
    "    join3[\"*\"],\n",
    "    df_aggregated_ca[\"ca_crime_count\"]\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "join4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3097108-5fe0-4689-b625-d458a4537c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+-----+--------------+--------------------+----+------+-------------+---------+------+----------+-----+------+------+----------+--------+------------+------+\n",
      "|      Date|REF_DATE| GEO|DGUID|Month_of_birth|     Characteristics| UOM|UOM_ID|SCALAR_FACTOR|SCALAR_ID|VECTOR|COORDINATE|VALUE|STATUS|SYMBOL|TERMINATED|DECIMALS|Day_of_birth|Births|\n",
      "+----------+--------+----+-----+--------------+--------------------+----+------+-------------+---------+------+----------+-----+------+------+----------+--------+------------+------+\n",
      "|1991-01-01|    1991|NULL| NULL|       January|Number of live bi...|NULL|  NULL|         NULL|     NULL|  NULL|      NULL| NULL|  NULL|  NULL|      NULL|    NULL|         1.0|1083.0|\n",
      "|1991-01-02|    1991|NULL| NULL|       January|Number of live bi...|NULL|  NULL|         NULL|     NULL|  NULL|      NULL| NULL|  NULL|  NULL|      NULL|    NULL|         2.0|1015.0|\n",
      "|1991-01-03|    1991|NULL| NULL|       January|Number of live bi...|NULL|  NULL|         NULL|     NULL|  NULL|      NULL| NULL|  NULL|  NULL|      NULL|    NULL|         3.0|1016.0|\n",
      "|1991-01-04|    1991|NULL| NULL|       January|Number of live bi...|NULL|  NULL|         NULL|     NULL|  NULL|      NULL| NULL|  NULL|  NULL|      NULL|    NULL|         4.0|1051.0|\n",
      "|1991-01-05|    1991|NULL| NULL|       January|Number of live bi...|NULL|  NULL|         NULL|     NULL|  NULL|      NULL| NULL|  NULL|  NULL|      NULL|    NULL|         5.0| 973.0|\n",
      "+----------+--------+----+-----+--------------+--------------------+----+------+-------------+---------+------+----------+-----+------+------+----------+--------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ca_births = spark_local.read.csv('/home/azureuser/data/projectdata/CanadianBirthFinal.csv', header=True, inferSchema=True)\n",
    "\n",
    "ca_births.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd3c1d3e-55d0-43ac-9b1c-f7c9960a87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 189:===================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+--------------+---------+\n",
      "|      Date|ca_longitude|ca_latitude|ca_max_temp|ca_min_temp|ca_mean_temp|ca_province|us_longitude|us_latitude|us_max_temp|us_min_temp|us_mean_temp|us_state|us_crime_count|us_births|ca_crime_count|ca_births|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+--------------+---------+\n",
      "|2000-01-06|      -78.18|      43.97|        3.5|      -10.0|        -3.3|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|    909.0|\n",
      "|2000-01-05|      -78.18|      43.97|        0.0|       -9.0|        -4.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|    901.0|\n",
      "|2000-01-02|      -78.18|      43.97|        9.0|       -2.0|         3.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|    889.0|\n",
      "|2000-01-01|      -78.18|      43.97|        6.0|       -5.0|         0.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|    833.0|\n",
      "|2000-01-04|      -78.18|      43.97|        9.0|       -1.0|         4.0|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|    831.0|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Perform the join and select the required columns\n",
    "join5 = join4.join(\n",
    "    ca_births, join4.Date == ca_births.Date, \"left_outer\"\n",
    ").select(\n",
    "    join4[\"*\"],\n",
    "    ca_births[\"Births\"].alias(\"ca_births\")\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "join5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f43e3e1-d4d6-471d-a7b4-47b650a76450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+---------------+------------------+--------+\n",
      "|      Date|Demand_Forecast_| Demand_|Net_Generation_|Total_Interchange_|    Sum_|\n",
      "+----------+----------------+--------+---------------+------------------+--------+\n",
      "|2016-01-01|        133710.0|120500.0|        97381.0|          -23119.0|-23119.0|\n",
      "|2016-01-02|        135707.0|120083.0|        95932.0|          -24151.0|-24151.0|\n",
      "|2016-01-03|        136047.0|120374.0|        93343.0|          -27031.0|-27031.0|\n",
      "|2016-01-04|        140562.0|124551.0|       107306.0|          -17245.0|-17245.0|\n",
      "|2016-01-05|        138014.0|125802.0|       113281.0|          -12521.0|-12521.0|\n",
      "+----------+----------------+--------+---------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usa_energy = spark_local.read.csv('/home/azureuser/data/projectdata/USEnergyConsumption.csv', header=True, inferSchema=True)\n",
    "\n",
    "usa_energy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f11e67b-82de-44a4-a524-52b557a23ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+--------------+---------+----------------+\n",
      "|      Date|ca_longitude|ca_latitude|ca_max_temp|ca_min_temp|ca_mean_temp|ca_province|us_longitude|us_latitude|us_max_temp|us_min_temp|us_mean_temp|us_state|us_crime_count|us_births|ca_crime_count|ca_births|us_energy_demand|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+--------------+---------+----------------+\n",
      "|2000-01-06|      -78.18|      43.97|        3.5|      -10.0|        -3.3|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|    909.0|            NULL|\n",
      "|2000-01-05|      -78.18|      43.97|        0.0|       -9.0|        -4.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|    901.0|            NULL|\n",
      "|2000-01-02|      -78.18|      43.97|        9.0|       -2.0|         3.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|    889.0|            NULL|\n",
      "|2000-01-01|      -78.18|      43.97|        6.0|       -5.0|         0.5|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|    833.0|            NULL|\n",
      "|2000-01-04|      -78.18|      43.97|        9.0|       -1.0|         4.0|         ON|        NULL|       NULL|       NULL|       NULL|        NULL|    NULL|          NULL|     NULL|          NULL|    831.0|            NULL|\n",
      "+----------+------------+-----------+-----------+-----------+------------+-----------+------------+-----------+-----------+-----------+------------+--------+--------------+---------+--------------+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the join and select the required columns\n",
    "join6 = join5.join(\n",
    "    usa_energy, join5.Date == usa_energy.Date, \"left_outer\"\n",
    ").select(\n",
    "    join5[\"*\"],\n",
    "    usa_energy[\"Demand_\"].alias(\"us_energy_demand\")\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "join6.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "030c2ed9-6e4c-45de-ac07-ee051f214d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specify the output path\n",
    "output_path = \"home/azureuser/Desktop/join6_output\"\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "join6.write.csv(output_path, header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fdcffc-c352-42ba-9b97-03c67f9009fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark_hadoop.stop()\n",
    "spark_postgres.stop()\n",
    "spark_sqlserver.stop()\n",
    "spark_mongo.stop()\n",
    "spark_local.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
